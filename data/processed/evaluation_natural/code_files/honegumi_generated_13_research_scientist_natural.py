# Generated by Honegumi (https://arxiv.org/abs/2502.06815)
# %pip install ax-platform==0.4.3 matplotlib
import numpy as np
import pandas as pd
from ax.service.ax_client import AxClient, ObjectiveProperties
import matplotlib.pyplot as plt


obj1_name = "branin"


def branin(x1, x2):
    y = float(
        (x2 - 5.1 / (4 * np.pi**2) * x1**2 + 5.0 / np.pi * x1 - 6.0) ** 2
        + 10 * (1 - 1.0 / (8 * np.pi)) * np.cos(x1)
        + 10
    )

    return y


ax_client = AxClient()

ax_client.create_experiment(
    parameters=[
        {"name": "x1", "type": "range", "bounds": [-5.0, 10.0]},
        {"name": "x2", "type": "range", "bounds": [0.0, 10.0]},
    ],
    objectives={
        obj1_name: ObjectiveProperties(minimize=True),
    },
)


for i in range(19):

    parameterization, trial_index = ax_client.get_next_trial()

    # extract parameters
    x1 = parameterization["x1"]
    x2 = parameterization["x2"]

    results = branin(x1, x2)
    ax_client.complete_trial(trial_index=trial_index, raw_data=results)

best_parameters, metrics = ax_client.get_best_parameters()


# Plot results
objectives = ax_client.objective_names
df = ax_client.get_trials_data_frame()

fig, ax = plt.subplots(figsize=(6, 4), dpi=150)
ax.scatter(df.index, df[objectives], ec="k", fc="none", label="Observed")
ax.plot(
    df.index,
    np.minimum.accumulate(df[objectives]),
    color="#0033FF",
    lw=2,
    label="Best to Trial",
)
ax.set_xlabel("Trial Number")
ax.set_ylabel(objectives[0])

ax.legend()
plt.show()

# Generated benchmark: Ax-based comparison of EI, UCB, and Thompson Sampling on Branin-Hoo
# %pip install ax-platform==0.4.3 botorch==0.10.0 matplotlib numpy pandas
import math
import os
import warnings
from typing import Dict, List, Tuple

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from ax.modelbridge.generation_strategy import GenerationStep, GenerationStrategy
from ax.modelbridge.registry import Models
from ax.service.ax_client import AxClient, ObjectiveProperties
from botorch.acquisition.monte_carlo import qThompsonSampling
from botorch.acquisition.analytic import qExpectedImprovement
from botorch.acquisition.upper_confidence_bound import qUpperConfidenceBound

warnings.filterwarnings("ignore")


# ---------------------------
# Problem-specific definitions
# ---------------------------

# Search space: Branin-Hoo function domain
# x1 in [-5, 10], x2 in [0, 15]
parameter_defs = [
    {"name": "x1", "type": "range", "bounds": [-5.0, 10.0]},
    {"name": "x2", "type": "range", "bounds": [0.0, 15.0]},
]

# Objective name: minimize branin_value
objective_name = "branin_value"

# Deterministic Branin-Hoo function (min ~ 0.397887 on this domain)
def evaluate_branin_value(x1: float, x2: float) -> float:
    a = 1.0
    b = 5.1 / (4.0 * math.pi**2)
    c = 5.0 / math.pi
    r = 6.0
    s = 10.0
    t = 1.0 / (8.0 * math.pi)
    return a * (x2 - b * x1**2 + c * x1 - r) ** 2 + s * (1 - t) * math.cos(x1) + s


BRANIN_GLOBAL_MIN = 0.39788735772973816  # reference global optimum


# ---------------------------
# Generation strategy builders
# ---------------------------

def make_generation_strategy(
    af_name: str,
    n_init: int,
    seed: int,
    ucb_beta: float = 0.2,
) -> GenerationStrategy:
    # Initialization: Sobol
    sobol_step = GenerationStep(
        model=Models.SOBOL,
        num_trials=n_init,
        max_parallelism=1,
        min_trials_observed=0,
        enforce_num_trials=True,
        model_kwargs={"seed": seed},
    )

    # Bayesian step with specified acquisition function
    if af_name.upper() == "EI":
        botorch_acqf_class = qExpectedImprovement
        acquisition_options = {}
    elif af_name.upper() == "UCB":
        botorch_acqf_class = qUpperConfidenceBound
        acquisition_options = {"beta": float(ucb_beta)}
    elif af_name.upper() in {"TS", "THOMPSON"}:
        botorch_acqf_class = qThompsonSampling
        acquisition_options = {}
    else:
        raise ValueError(f"Unknown acquisition function: {af_name}")

    bo_step = GenerationStep(
        model=Models.BOTORCH_MODULAR,
        num_trials=-1,
        max_parallelism=1,
        model_kwargs={
            "botorch_acqf_class": botorch_acqf_class,
            "acquisition_options": acquisition_options,
        },
    )

    return GenerationStrategy(steps=[sobol_step, bo_step])


# ---------------------------
# Single-run executor
# ---------------------------

def run_single_branin(
    af_name: str,
    n_init: int,
    n_opt: int,
    seed: int,
    ucb_beta: float = 0.2,
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Run one independent optimization for a given acquisition function.

    Returns:
      - observed_values: shape (n_init + n_opt,)
      - best_so_far: cumulative minimum values, same shape
    """
    np.random.seed(seed)

    gs = make_generation_strategy(af_name=af_name, n_init=n_init, seed=seed, ucb_beta=ucb_beta)
    ax_client = AxClient(generation_strategy=gs, random_seed=seed)
    ax_client.create_experiment(
        name=f"branin_{af_name}_seed{seed}",
        parameters=parameter_defs,
        objectives={objective_name: ObjectiveProperties(minimize=True)},
    )

    total_trials = n_init + n_opt
    observed = np.zeros(total_trials, dtype=float)

    for i in range(total_trials):
        params, trial_index = ax_client.get_next_trial()
        x1 = float(params["x1"])
        x2 = float(params["x2"])
        y = evaluate_branin_value(x1, x2)
        # Deterministic: SEM = 0.0
        ax_client.complete_trial(trial_index=trial_index, raw_data=(y, 0.0))
        observed[i] = y

    best_so_far = np.minimum.accumulate(observed)
    return observed, best_so_far


# ---------------------------
# Benchmark runner
# ---------------------------

def run_benchmark_suite(
    acquisition_functions: List[str],
    n_init: int,
    n_opt: int,
    n_repeats: int,
    ucb_beta: float = 0.2,
    base_seed: int = 12345,
) -> Dict[str, Dict[str, np.ndarray]]:
    """
    Run multiple independent seeds per acquisition function.

    Returns dict:
      results[af] = {
         "observed": array shape (n_repeats, n_trials),
         "best_so_far": array shape (n_repeats, n_trials),
         "regret": array shape (n_repeats, n_trials),
      }
    """
    n_trials = n_init + n_opt
    results: Dict[str, Dict[str, np.ndarray]] = {}

    for af in acquisition_functions:
        observed_all = np.zeros((n_repeats, n_trials), dtype=float)
        best_all = np.zeros_like(observed_all)
        for r in range(n_repeats):
            seed = base_seed + r
            observed, best_so_far = run_single_branin(
                af_name=af, n_init=n_init, n_opt=n_opt, seed=seed, ucb_beta=ucb_beta
            )
            observed_all[r, :] = observed
            best_all[r, :] = best_so_far

        regret_all = best_all - BRANIN_GLOBAL_MIN
        results[af] = {
            "observed": observed_all,
            "best_so_far": best_all,
            "regret": regret_all,
        }

    return results


# ---------------------------
# Visualization utilities
# ---------------------------

def mean_and_ci(arr: np.ndarray, axis: int = 0, alpha: float = 0.05) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    Compute mean and two-sided (1-alpha) CI using normal approx: mean Â± z * SEM.
    """
    mean = arr.mean(axis=axis)
    std = arr.std(axis=axis, ddof=1)
    n = arr.shape[axis]
    z = 1.96  # approx for 95% CI
    sem = std / np.sqrt(n)
    lower = mean - z * sem
    upper = mean + z * sem
    return mean, lower, upper


def plot_benchmark(results: Dict[str, Dict[str, np.ndarray]], ylabel: str, key: str) -> None:
    plt.figure(figsize=(8, 5), dpi=150)
    for af, data in results.items():
        y = data[key]
        mean, lower, upper = mean_and_ci(y, axis=0)
        x = np.arange(1, y.shape[1] + 1)
        plt.plot(x, mean, lw=2, label=af)
        plt.fill_between(x, lower, upper, alpha=0.15)
    plt.xlabel("Trial number")
    plt.ylabel(ylabel)
    plt.legend()
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.show()


# ---------------------------
# Main
# ---------------------------

if __name__ == "__main__":
    # Configure experiment scale; reduce if SMOKE_TEST is set
    SMOKE = os.environ.get("SMOKE_TEST", "").lower() in {"1", "true", "yes"}
    N_INIT = 8 if not SMOKE else 3
    N_OPT = 42 if not SMOKE else 7
    N_REPEATS = 15 if not SMOKE else 3
    UCB_BETA = 0.2  # exploration parameter for UCB

    acquisition_functions = ["EI", "UCB", "TS"]

    print(f"Running Branin benchmark with acquisition functions: {acquisition_functions}")
    print(f"Initialization points: {N_INIT}, optimization iterations: {N_OPT}, repeats: {N_REPEATS}")

    results = run_benchmark_suite(
        acquisition_functions=acquisition_functions,
        n_init=N_INIT,
        n_opt=N_OPT,
        n_repeats=N_REPEATS,
        ucb_beta=UCB_BETA,
        base_seed=2025,
    )

    # Summary printout
    n_trials = N_INIT + N_OPT
    print("\nFinal best-so-far summary (lower is better):")
    summary_rows = []
    for af, data in results.items():
        final_best = data["best_so_far"][:, -1]
        mean = final_best.mean()
        std = final_best.std(ddof=1)
        summary_rows.append({"Acquisition": af, "Final best mean": mean, "Final best std": std})
    summary_df = pd.DataFrame(summary_rows).sort_values("Final best mean")
    print(summary_df.to_string(index=False, float_format=lambda v: f"{v:0.6f}"))

    # Plots
    plot_benchmark(results, ylabel="Best-so-far objective (branin_value)", key="best_so_far")
    plot_benchmark(results, ylabel="Best-so-far regret vs. global min", key="regret")