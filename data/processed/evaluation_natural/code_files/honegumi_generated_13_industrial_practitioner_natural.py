# Generated by Honegumi (https://arxiv.org/abs/2502.06815)
# %pip install ax-platform==0.4.3 matplotlib
import numpy as np
import pandas as pd
from ax.service.ax_client import AxClient, ObjectiveProperties
import matplotlib.pyplot as plt


obj1_name = "branin"


def branin(x1, x2):
    y = float(
        (x2 - 5.1 / (4 * np.pi**2) * x1**2 + 5.0 / np.pi * x1 - 6.0) ** 2
        + 10 * (1 - 1.0 / (8 * np.pi)) * np.cos(x1)
        + 10
    )

    return y


ax_client = AxClient()

ax_client.create_experiment(
    parameters=[
        {"name": "x1", "type": "range", "bounds": [-5.0, 10.0]},
        {"name": "x2", "type": "range", "bounds": [0.0, 10.0]},
    ],
    objectives={
        obj1_name: ObjectiveProperties(minimize=True),
    },
)


for i in range(19):

    parameterization, trial_index = ax_client.get_next_trial()

    # extract parameters
    x1 = parameterization["x1"]
    x2 = parameterization["x2"]

    results = branin(x1, x2)
    ax_client.complete_trial(trial_index=trial_index, raw_data=results)

best_parameters, metrics = ax_client.get_best_parameters()


# Plot results
objectives = ax_client.objective_names
df = ax_client.get_trials_data_frame()

fig, ax = plt.subplots(figsize=(6, 4), dpi=150)
ax.scatter(df.index, df[objectives], ec="k", fc="none", label="Observed")
ax.plot(
    df.index,
    np.minimum.accumulate(df[objectives]),
    color="#0033FF",
    lw=2,
    label="Best to Trial",
)
ax.set_xlabel("Trial Number")
ax.set_ylabel(objectives[0])

ax.legend()
plt.show()

# %pip install ax-platform==0.4.3 matplotlib
import math
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import warnings
from typing import Callable, Dict, List, Tuple

import torch
from ax.service.ax_client import AxClient, ObjectiveProperties
from ax.modelbridge.generation_strategy import GenerationStep, GenerationStrategy
from ax.modelbridge.registry import Models

# BoTorch acquisition functions to compare
from botorch.acquisition.monte_carlo import qExpectedImprovement, qNoisyExpectedImprovement, qUpperConfidenceBound
from botorch.acquisition.logei import qLogNoisyExpectedImprovement


# -------------------------
# Branin benchmark (objective: minimize branin_value)
# -------------------------
def evaluate_branin(parameters: Dict[str, float], noise_sd: float = 0.0) -> float:
    """
    Branin function evaluation.

    The standard Branin-Hoo function domain:
      - branin_x1 in [-5, 10]
      - branin_x2 in [0, 15]

    Returns:
      Single float value corresponding to "branin_value".
      If noise_sd > 0, adds Gaussian noise N(0, noise_sd^2) to simulate measurement noise.
    """
    x1 = float(parameters["branin_x1"])
    x2 = float(parameters["branin_x2"])

    a = 1.0
    b = 5.1 / (4.0 * math.pi**2)
    c = 5.0 / math.pi
    r = 6.0
    s = 10.0
    t = 1.0 / (8.0 * math.pi)
    y = a * (x2 - b * x1**2 + c * x1 - r) ** 2 + s * (1.0 - t) * math.cos(x1) + s

    if noise_sd > 0.0:
        y += np.random.normal(loc=0.0, scale=noise_sd)

    return float(y)


# -------------------------
# Setup utility: GenerationStrategy for a given acquisition function
# -------------------------
def make_generation_strategy(
    n_sobol: int,
    sobol_seed: int,
    botorch_acqf_class,
    acquisition_options: Dict = None,
) -> GenerationStrategy:
    """
    Create a two-step generation strategy:
      1) Sobol initialization for n_sobol trials
      2) GP-based BO using the specified BoTorch acquisition function
    """
    acq_kwargs = {"botorch_acqf_class": botorch_acqf_class}
    if acquisition_options:
        acq_kwargs["acquisition_options"] = acquisition_options

    gs = GenerationStrategy(
        steps=[
            GenerationStep(
                model=Models.SOBOL,
                num_trials=n_sobol,
                min_trials_observed=n_sobol,
                max_parallelism=n_sobol,
                model_kwargs={"seed": sobol_seed},
            ),
            GenerationStep(
                model=Models.BOTORCH_MODULAR,
                num_trials=-1,
                max_parallelism=1,  # sequential (batch size = 1)
                model_kwargs=acq_kwargs,
            ),
        ]
    )
    return gs


# -------------------------
# Run a single AxClient experiment for a chosen acquisition
# -------------------------
def run_single_optimization(
    acqf_name: str,
    acqf_class,
    total_trials: int,
    n_sobol_init: int,
    seed: int,
    noise_sd: float = 0.0,
    acquisition_options: Dict = None,
) -> Dict:
    """
    Runs one optimization loop with the specified acquisition function.

    Returns a dict with:
      - "trace_best_observed": list of best observed branin_value per trial
      - "best_observed_final": float, final best observed value
      - "best_predicted_final": float, model-predicted best at the end
    """
    # Reproducibility
    np.random.seed(seed)
    torch.manual_seed(seed)

    # Build generation strategy
    gs = make_generation_strategy(
        n_sobol=n_sobol_init,
        sobol_seed=seed,
        botorch_acqf_class=acqf_class,
        acquisition_options=acquisition_options,
    )

    ax_client = AxClient(generation_strategy=gs, verbose_logging=False)

    # Create experiment
    ax_client.create_experiment(
        name=f"branin_benchmark_{acqf_name}_seed{seed}",
        parameters=[
            {"name": "branin_x1", "type": "range", "bounds": [-5.0, 10.0], "value_type": "float"},
            {"name": "branin_x2", "type": "range", "bounds": [0.0, 15.0], "value_type": "float"},
        ],
        objectives={"branin_value": ObjectiveProperties(minimize=True)},
    )

    trace_best_observed: List[float] = []
    best_so_far = float("inf")

    for _ in range(total_trials):
        params, trial_index = ax_client.get_next_trial()
        y = evaluate_branin(params, noise_sd=noise_sd)
        ax_client.complete_trial(trial_index=trial_index, raw_data=y)

        # Track best observed so far (raw observed objective values)
        best_so_far = min(best_so_far, y)
        trace_best_observed.append(best_so_far)

    # Model-predicted best at the end of the run
    try:
        _, best_values = ax_client.get_best_parameters()
        means, _ = best_values
        best_pred = float(means.get("branin_value", np.nan))
    except Exception:
        best_pred = float("nan")

    return {
        "trace_best_observed": trace_best_observed,
        "best_observed_final": trace_best_observed[-1],
        "best_predicted_final": best_pred,
    }


# -------------------------
# Aggregate experiments across multiple replications for each acquisition
# -------------------------
def compare_acquisitions(
    acquisitions: Dict[str, Tuple[Callable, Dict]],
    total_trials: int = 40,
    n_sobol_init: int = 8,
    n_repetitions: int = 10,
    base_seed: int = 12345,
    noise_sd: float = 0.0,
) -> Dict[str, Dict]:
    """
    acquisitions: dict mapping acquisition name -> (BoTorch acqf class, acquisition_options dict)
    Returns a dict mapping acquisition name to:
      - "traces": np.ndarray of shape (n_repetitions, total_trials)
      - "best_observed": np.ndarray of shape (n_repetitions,)
      - "best_predicted": np.ndarray of shape (n_repetitions,)
    """
    results = {}

    for acqf_name, (acqf_class, acq_opts) in acquisitions.items():
        traces = np.zeros((n_repetitions, total_trials), dtype=float)
        best_observed = np.zeros(n_repetitions, dtype=float)
        best_predicted = np.zeros(n_repetitions, dtype=float)

        for rep in range(n_repetitions):
            seed = base_seed + rep
            outcome = run_single_optimization(
                acqf_name=acqf_name,
                acqf_class=acqf_class,
                total_trials=total_trials,
                n_sobol_init=n_sobol_init,
                seed=seed,
                noise_sd=noise_sd,
                acquisition_options=acq_opts,
            )
            traces[rep, :] = np.asarray(outcome["trace_best_observed"], dtype=float)
            best_observed[rep] = float(outcome["best_observed_final"])
            best_predicted[rep] = float(outcome["best_predicted_final"])

        results[acqf_name] = {
            "traces": traces,
            "best_observed": best_observed,
            "best_predicted": best_predicted,
        }

    return results


# -------------------------
# Plotting utilities
# -------------------------
def plot_optimization_traces(results: Dict[str, Dict], title: str = "Branin benchmark: best observed vs. trials"):
    plt.figure(figsize=(8, 5), dpi=150)
    for acqf_name, res in results.items():
        traces = res["traces"]
        mean_trace = traces.mean(axis=0)
        std_trace = traces.std(axis=0)
        trials = np.arange(1, mean_trace.shape[0] + 1)

        plt.plot(trials, mean_trace, lw=2, label=acqf_name)
        plt.fill_between(trials, mean_trace - std_trace, mean_trace + std_trace, alpha=0.15)

    plt.xlabel("Trial number")
    plt.ylabel("Best observed branin_value (lower is better)")
    plt.title(title)
    plt.grid(True, alpha=0.3)
    plt.legend()
    plt.tight_layout()
    plt.show()


def plot_final_boxplot(results: Dict[str, Dict], use_predicted: bool = False, title: str = "Final best performance"):
    data = []
    labels = []
    key = "best_predicted" if use_predicted else "best_observed"

    for acqf_name, res in results.items():
        data.append(res[key])
        labels.append(acqf_name)

    plt.figure(figsize=(8, 4), dpi=150)
    b = plt.boxplot(data, labels=labels, showmeans=True, patch_artist=True)
    for patch in b["boxes"]:
        patch.set_facecolor("#D8E6FF")
    plt.ylabel(f"Final {key.replace('_', ' ')} (lower is better)")
    plt.title(title)
    plt.grid(True, axis="y", alpha=0.3)
    plt.tight_layout()
    plt.show()


# -------------------------
# Main
# -------------------------
if __name__ == "__main__":
    warnings.filterwarnings("ignore")

    # Configuration
    TOTAL_TRIALS = 40         # total trials per repetition (including initialization)
    N_SOBOL_INIT = 8          # number of Sobol initialization trials
    N_REPETITIONS = 10        # number of independent runs per acquisition to gauge robustness
    BASE_SEED = 20251031      # seed base for reproducibility across runs
    NOISE_SD = 0.0            # set > 0.0 to simulate noisy observations

    # Acquisition functions to compare (all with standard GP surrogate)
    # For UCB, 'beta' controls exploration (higher -> more exploration).
    ACQUISITIONS = {
        "qLogNEI (default)": (qLogNoisyExpectedImprovement, {}),
        "qNEI": (qNoisyExpectedImprovement, {}),
        "qEI": (qExpectedImprovement, {}),
        "qUCB(beta=2.0)": (qUpperConfidenceBound, {"beta": 2.0}),
    }

    # Run comparison
    results_by_acq = compare_acquisitions(
        acquisitions=ACQUISITIONS,
        total_trials=TOTAL_TRIALS,
        n_sobol_init=N_SOBOL_INIT,
        n_repetitions=N_REPETITIONS,
        base_seed=BASE_SEED,
        noise_sd=NOISE_SD,
    )

    # Summarize to DataFrame
    summary_rows = []
    for acqf_name, res in results_by_acq.items():
        obs = res["best_observed"]
        pred = res["best_predicted"]
        summary_rows.append(
            {
                "acquisition": acqf_name,
                "final_best_observed_mean": np.mean(obs),
                "final_best_observed_std": np.std(obs),
                "final_best_predicted_mean": np.nanmean(pred),
                "final_best_predicted_std": np.nanstd(pred),
            }
        )
    summary_df = pd.DataFrame(summary_rows).sort_values("final_best_observed_mean")
    print("\nSummary of final performance across repetitions (lower is better):")
    print(summary_df.to_string(index=False))

    # Plots
    plot_optimization_traces(results_by_acq, title="Branin benchmark: mean best-observed trace ±1 std")
    plot_final_boxplot(results_by_acq, use_predicted=False, title="Final best observed distribution by acquisition")
    plot_final_boxplot(results_by_acq, use_predicted=True, title="Final best predicted (model) distribution by acquisition")